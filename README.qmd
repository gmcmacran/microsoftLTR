---
format:
  gfm:
    html-math-method: webtex
jupyter: python3
---

# Summary
This repo builds a boosted model focused on ranking with LightGPM on the Microsoft's Web30K dataset. Gain at the top 10 scores is around .583 on unseen data. This is considerably higher than the publication this code is based on indicating improvements in source code after printing.

For compute speed, I used the Polars data frame library and GPU training.

# Data Overview

```{python}
#| include: false
import os
import polars as pl
import numpy as np
from plotnine import ggplot, aes, labs, geom_boxplot, scale_y_continuous

os.chdir('S:/Python/projects/semi_supervised')

folder = 'S:/Python/projects/microsoftLTR/data/'
train = pl.read_ipc(folder + 'train.ipc')
valid = pl.read_ipc(folder + 'vali.ipc')
test = pl.read_ipc(folder + 'test.ipc')
del folder
train.head()

halfIndex = int(np.floor(valid.shape[0] / 2))
train = pl.concat([train, valid[0:halfIndex]])
test = pl.concat([valid, valid[halfIndex:valid.shape[0]]])
del valid, halfIndex
```

## Query and Documents
Learning to rank (L.T.R.) M.L. has its origin in search engine optimization. Because of this, there are two key ideas. Documents and queries. A document is a web page. The page is crawled and features are created. Example features include covered query term number, term frequency, and stream length. A query is the string the user types into Bing. There is a many to one relationship between documents and queries stemming from the user sees many web pages per search. Further, a document may show up in many queries.

Each row in training is a document. The first ten rows look like

```{python}
#| echo: false
print(train.head())
```

-   Train N: 2,643,905 (documents)
-   Test N: 1,120,827 (documents)

-   Train queries: 22,036
-   Test queries: 6,306

For these data, the number of documents per query looks somewhat similar.

```{python}
#| echo: false
temp = (
    pl.concat([
        train.
        groupby('qid').
        agg([pl.col('label').count().alias('documentCount')]).
        with_column(pl.lit('train').alias('dataset')),
        test.
        groupby('qid').
        agg([pl.col('label').count().alias('documentCount')]).
        with_column(pl.lit('test').alias('dataset')),
    ]).
    to_pandas()
)
(
    ggplot(temp, aes(x = 'dataset', y = 'documentCount'))
    + geom_boxplot()
    + scale_y_continuous(breaks = np.arange(0, 2000, 100))
    + labs(x = "Dataset", y = "Count Of Documents Per Query")
)
```

This project uses fold one of Microsoft's data. The vali.txt file is split into two pieces and put into train and test datasets. Raw data can be found [here](https://www.microsoft.com/en-us/research/project/mslr/).

## Cleaning Process
-   Step 1: Load .txt files.
-   Step 2: Name columns.
-   Step 3: Remove a few columns.
-   Step 4: Remove leading string in values.
-   Step 5: Convert variables to numeric.
-   Step 6: Write ipc files for quick loads.

# Model Summary
## Results
Model training is made easy by LightGPM. For ranking the major changes are calling LGBMRanker and setting the objective to "rank_xendcg". Without much tuning, the model had a NCDG of .58 for top 5 scores and .58 for top 10 scores on the unseen test data. 

For reference, the publication An Alternative Cross Entropy Loss for Learning-to-Rank has NCDG around .48 and this was bleeding edge performance when it was printed. Microsoft has improved model training since then.

## Cross Validation Considerations

There are two main challenges: 

-   1: Different data between calling predict and performance calculations. 
-   2: Keeping all documents in a query ID together for in sample and out of sample assessment.

For point one, the data passed into LGBMRanker has one row per document. The performance metric requires one row per query and document scores are the columns. This intermediate data shaping between predictions to performance calculations requires a custom metric creation to use scikit-learn’s cross validation functionality.

Point two is the nail in the coffin for scikit-learn’s CV functionality. Learning to rank data requires care to keep all documents within a query together. Scikit-learn assumes each row is independent and takes a random samples. This is true for regression and classification but is not in learning to rank problems.

The path forward is either writing nested for loops or using an autoML library like FLAML.

# Tech Stack
## Polars Data Frame Library
Polars is a high speed data frame library capable of handling millions in memory data points. H2O’s ran a benchmark across python’s, R’s, and Julia’s data frame libraries. Polars was often the fastest and almost always in the top 3. It usually beats R’s data.table.

## GPU Considerations
For the Windows platform, the pip install process includes all the necessary parts for training on a GPU. At writing, the Linux and Mac don't have this functionality. In addition, installing with conda on Windows does not provide GPU support.

# Mathematical History
Learning to rank is a separate approach from regression and classification. Through loss functions, L.T.R. focuses on optimizing a rank based statistic during training. Example rank based statistics are mean reciprocal rank (MRR), mean average precision (MAP), and normalized discounted cumulative gain (NDCG). Fundamentally, optimizing rank statistics is challenging because ranking is a discontinuous process and any single point’s rank depends on all other points’ scores. Many approaches have been purposed to deal with these challenges.

One example is approximating the ranking statistic with a smooth continuous function and optimizing the approximation with typical gradient based approaches. This approach is only moderately successful as the smoother the function is (and therefore easier to optimize), the less accurate the approximation becomes.

Another approach is formulating a lower bound on a ranking statistic that is smooth and optimizing it. Bounding has two problems. The optimal value of the bound is not guaranteed to be the optimal value of the rank statistic. Further, increases in the bound may be associated with no change in the rank statistic.

LambdaRank was a major breakthrough in performance. Empirically, this loss function lead to better rank statistics than any other loss function of the time. It was the first rank focused loss function to get high quality implementations in both LightGBM and XGBoost. The one drawback was it was not mathematically well understood and relied on heuristics.

XE-NDCG is a leap forward because it provides a clear mathematical framework, is convex and is differentiable. These properties lead to improvements in rank statistics, easier optimization during training, and improved robustness to mislabeled data.